\section[Backgroud on Quantum Metrology]
{Background on Quantum Metrology}
\input{snp/singleLineWaterMark.tex}

\quotes{Roger J. Barlow}{In the real world, doing real experiments, statistics began to matter}

\vspace{0pt}
\lettrine[lines=2, findent=3pt,nindent=0pt]{I}{n} this chapter we will study the basics of Quantum Metrology, which stands for the science of enhanced metrology by quantum phenomena.
On the other hand, metrology, as the science of measuring, has played an essential role for the development of the technology as we know it today.
It studies several aspects of the estimation process, such as which strategy to follow in order to improve the precision of an estimation or which is the bound for the precision.
Metrology also covers all intermediate process for the estimation, from the design aspects of a precise measuring device, to the most basic concepts of estimation, which lead at the end to a better understanding of it as a whole.

In this sense, with the discovery of the Quantum Physics and the development of Quantum Mechanics, new doors for advances in metrology were open in the early decades of the 20th century.
First, Quantum Theory embraced the so-called field of Quantum Information, which merges the notions of the theory of information and computer science, among others sub-fields, with the quantum mechanics.
This new field of Quantum Information is in the basis of Quantum Metrology.
Moreover, those emerging fields rapidly became into very interesting interdisciplinary playgrounds of science attracting the interest of many scientists as well as resources for their researches.
Just to mention that the role of the so-called entanglement, an exclusive feature of Quantum Mechanics which cannot be completely described using classical probabilistic theories, is essential in this context.
With the aim of completely understand this concepts, many scientists world wide have integrated their efforts.
Said this, the entanglement also is in the center of theoretical concepts in Quantum Metrology.
In the Quantum Metrology framework, we will focus on the achievable precision of the systems.
We will show as well different strategies to achieve the desired results.

On the other hand, a very important field of science must be highlighted by its own merits.
We are mentioning the statistics, without which many descriptions of the actual and past physical findings would lack of the rigorous interpretation needed for the complexity of data samples.
It basically helps to analyse raw data to find special properties out of it.

\subsection{Background on statistics and the theory of estimation}

In this section, we will enumerate the basic concepts of statistics as well as the estimation process.
As we said, the main mathematical tools used by the metrology science belong to statistics.
Moreover, we are especially interested in estimation theory.
The statistics main characteristic is that it makes the raw data under consideration comprehensible from the human perspective.
The data can be anything, from a set of different heights of a basketball team, to the outcomes of a coin toss, or the wavelength of photons coming out of some sample.
The aim of this section is to give the reader sufficient material to follow this thesis and make it comprehensible from the beginning\footnote{
For a more detailed material, check statistics book as well as mathematics book for scientist and engineers \citep{Riley1998, BarlowXXXX}}.

\subsubsection{Probability, data samples, average and variance}

The probability indicates the relative chance of an event to happen.
For instance, if there is a box with ten red balls and five blue balls, considering that the balls of the same color are indistinguishable and that we extract one randomly, we have $\frac{5}{10+5}=\frac{1}{3}$ of probability to obtain a blue ball and $\frac{2}{3}$ to obtain a red ball.
The reader may notice many properties a probability should have, such as that the probability of any event to happen is always one or that a probability is always given by a number in between zero and one.
For more details, there are many interesting textbooks or chapters of some textbook about Probability Theory one could follow if interested.
The following References~\citep{Riley1998, BarlowXXXX} are some of those books.

When someone has a data sample at hand, it might come from diverse sources and might be represented using multiple forms such as numbers or words (for instance, a table of names of people), the first thing one tries to accomplish is to analyze the data to extract the relevant properties from it.
A data sample always come from a population sample, i.e., the data sample might not be complete (one could lose some in the process) neither exact (a measuring device always have an error when estimating a magnitude).
Hence, the data sample comes attached with a probability for each data element.
In the subsequent paragraphs, we will describe this relation and we will enumerate the most useful properties and formulas for the comprehensions of this thesis.

First of all, we will explain our notation which follows the one used on Reference~\citep{Riley1998}.
A probability function gives the probability of an event $x$ to happen when measuring some random variable $X$, and it is denoted by $\prob(X=x)$.
Second, the $N$ elements of a data sample are considered to come from $N$ random variables with the corresponding $N$ values due to the random nature of the measuring, $\{X_i=x_i\}_{i=1}^N$.
The joint probability of those random variables is in general not separable.
This is due to the fact that the outcomes could depend on the rest of the outcomes or some other more complex relation that makes the most general case to be non-separable from the probabilistic point of view.
Therefore we define this PDF as an $N$ variable function
\be
  \prob(\bs{X}=\bs{x}) \equiv \prob(X_1=x_1,X_2=x_2,\dots,X_N=x_N).
\ee
If separable this is written by
\be
  \prob(\bs{X}=\bs{x}) = \prod_{i=1}^N\prob(X_i=x_i)
\ee
and it is the case for many relevan cases.

When some property of the system is defined as a function of random variables, the result is also a random variable with another transformed PDF.
For example, we measure the position of a body at some moment.
If the system was at rest on the origin at initial time and the acceleration is known to be constant, then from the position one can infer on the value of the acceleration by using $A=2X/t^2$, where $X$ denotes the position at instant $t$ and $A$ the acceleration.
If $X$ is a random variable, which is the general case when measuring the position of a body, then the probability is computed in this case by
\be
  \prob(A=a) = \frac{\text{d}x}{\text{d}a}\prob(X=x)=\frac{2}{t^2}\prob(X=x).
\ee
In general for the multiple random variable case transformations, we must require that
\be
  |\prob(\bs{X}=\bs{x})\,\text{d}x_1\text{d}x_2\dots\text{d}x_N|=|\prob(\bs{Y}=\bs{y})\,\text{d}y_1\text{d}y_2\dots\text{d}y_N|.
\ee
This leads to some interesting formulas we will discuss later.

We now stick to the simplest case on which the data is a collection of values of the same random variable describing the same physical one-dimensional data population.
Some definitions reasonable to mention in this thesis arise for those kind of data samples: the average, variance, moments and central moments.
The arithmetic average (there are other types of average one can find in the textbooks) is computed by
\be
  \overline{x}=\frac{1}{N}\sum x_i.
\ee
The variance is related with the spread of the data and computed by
\be
  \sigma^2=\frac{1}{N}\sum (x_i-\overline{x})^2,
\ee
where $\sigma$ is the standard deviation.
Different moments are computed by $\overline{x^r}=\frac{1}{N}\sum x_i^r$ while central moments are of the form of $n_r=\frac{1}{N}\sum (x_i-\overline{x})^r$.
Notice that the variance is the second central moment of the data sample.
For completeness, when each element of the data consists of more than a number, e.g., $(x_i,y_i,z_i)$, the co-variance between two data kinds, in the following case $X$ and $Y$, is obtained by
\be
  V_{X,Y} = \frac{1}{N}\sum_{i=1}^N (x_i-\overline{x})(y_i-\overline{y}).
\ee

Those functions also apply to the probability distribution functions.
While these last ones may be unknown functions for us, as soon as we collect all the possible data or we are in the large number regime the variances of the data sample and of the data population, the means, the co-variances and so on so forth, they must coincide.
We will try to keep this distinction as clear as possible.
In general the mean values of any function over data sample values $g(x)$ are denoted with a bar over a lowercase variable, e.g., $\overline{x^r}$ for the $r$-th moment, while the mean values of any function applied to the data population $g(X)$ is written following some textbooks by $\text{E}[g(X)]$.
One clearly may distinguish two cases here: one for continuous functions where the expectation values or the means are computed integrating over all the possible values, and the other case on which $\prob(\bs{X}=\bs{x})$ can take only discrete values.
For completeness here are the two definitions
\be
  \text{E}[g(\bs{X})] = \lcor
  \begin{split}
    &\int g(\bs{x}) \prob(\bs{X}=\bs{x})\,\text{d}^N \bs{x},\\
    &\sum_{i,j,\dots} g(\bs{x}) \prob(\bs{X}=\bs{x}).
  \end{split}
  \right.
\ee
One more definition needs our attention and it is the variance any function $g(X)$ denoted by $\text{V}[g(X)] \equiv \text{E}[g(X)^2] - E[g(X)]^2$.

\subsubsection{Frecuentist vs. bayesian approach}

\subsubsection{Estimators and Fisher information}

Let us suppose that the data sample has encoded some wanted parameters $\bs{a}\equiv(a_1,a_2,\dots)$.
The underlying probability, in general also unknown, may be conditioned by the real values of the wanted parameters $\bs{a}$.
The probability of the data sample is therefore written as
\be
  \prob(\bs{X}=\bs{x}|\bs{a}),
\ee
where "$|\bs{a}$" indicates its dependency on these parameters.
At this point, notice that an estimate of the the wanted parameter based on the data sample has the form of a function the random variables.
Therefore as mentioned before, an estimator of $a$, which is how is called this quantity and is denoted by $\hat{a}$, is another random variable with a PDF of the form of
\be
  \prob(\hat{a}|\bs{a})\,\text{d}\hat{a} = \prob(\bs{X}=\bs{x}|\bs{a})\,\text{d}x_1\text{d}x_2\dots\text{d}x_N.
\ee
For short, we omit on writing $\bs{X}$ from now on, thus the conditional joint probability of $N$ random variables is written as $\prob(\bs{x}|\bs{a})$.

As we said, an estimator is defined to be a function of the data sample.
For example, one of such estimators is the estimator of the mean value of the data population, in general unknown.
The mean value of the data population, which in general we do not have access to, is denoted usually by $\mu$.
Notice that $\mu$ is in general different from the mean value of the data sample $\overline{x}$.
A valid estimator for the population mean value would be the arithmetic mean of the data sample, i.e., $\hat{\mu}=\overline{x}$.

An important notion of an estimator is its \emph{efficiency}, i.e., whether it has smaller variance than another.
The more efficient estimator the smaller variance it has.
Remember than an estimator is a random variable so it must have a variance that comes from the data population, i.e., from $\prob(\hat{a}|\bs{a})$ in this case.

For an estimator of any kind there is a theoretical lower bound for its variance.
For the proof of the previous observation, which we compute for continuous random variables without losing of generality, we start with the normalization of the PDF
\be
  \int \prob(\bs{x}|\bs{a})\,\text{d}^N\bs{x} = 1.
\ee
Next, we compute the partial derivative over $a$ such that
\be
  \int \partial_a\prob(\bs{x}|\bs{a})\,\text{d}^N\bs{x} = \int \partial_a\big(\ln  \prob(\bs{x}|\bs{a})\big) \prob(\bs{x}|\bs{a}) \,\text{d}^N\bs{x} = 0,
\ee
where for simplification, we omit the arguments of the joint probability on the second equality.
From Eq.~\eqref{}, it turns out that the Eq.~\eqref{} is the expectation value of $\partial_a(\ln\prob)$.
Finally if we have an unbiased estimator, for which the true value $a$ coincides with the expectation value of the estimator $\text{E}[\hat{a}]$, its partial differentiation over $a$ must be equal to one,
\begin{align}
  \partial_a\text{E}[\hat{a}] &= \partial_a a = 1, \\
  \begin{split}
    1 &= \partial_a \int \hat{a} \prob(\bs{x}|\bs{a})\,\text{d}^N\bs{x} \\
      &=  \int \hat{a} \partial_a\prob(\bs{x}|\bs{a})\,\text{d}^N\bs{x} =  \int \hat{a} \partial_a\big(\ln  \prob(\bs{x}|\bs{a})\big) \prob(\bs{x}|\bs{a}) \,\text{d}^N\bs{x},
  \end{split}
\end{align}
where we apply the definition of the expectation value of the estimator $\hat{a}$ for continuous variables and we apply as well similar identities as in Eq.~\eqref{}.

At this point we invoke the Schwartz inequality for two real multidimensional functions $g(\bs{x})$ and $h(\bs{x})$ such that $(\int g h \,\text{d}^N\bs{x})^2\leq (\int g^2 \,\text{d}^N\bs{x}) (\int h^2 \,\text{d}^N\bs{x})$.
With this we can follow the following recipe to obtain a lower bound for the variance of a general estimator.
First the definition of the variance looks like
\be
  V[\hat{a}] = E[(\hat{a}-a)^2] = \int (\hat{a} - a)^2\prob(\bs{x}|\bs{a})\,\text{d}^N\bs{x}.
\ee
Second, from Eqs.~\eqref{,} holds that
\be
  \int (\hat{a}-a)\partial_a\big(\ln  \prob(\bs{x}|\bs{a})\big) \prob(\bs{x}|\bs{a})\,\text{d}^N\bs{x} = 1
\ee
because $a$ is not a function of $\bs{x}$.
Hence, using the Schwartz inequality we can write
\be
  \label{eq:bg-classical-cr-bound-and-fi}
  \text{V}[\hat{a}] = \int (\hat{a} - a)^2\prob(\bs{x}|\bs{a})\,\text{d}^N\bs{x} \geq \frac{1}{\int \lcua\partial_a\big(\ln \prob (\bs{x}|\bs{a}) \big)\rcua^2 \prob (\bs{x}|\bs{a})\,\text{d}^N\bs{x}},
\ee
which is also known as the Cram\'er-Rao bound or the Fisher inequality.
The denominator on the right hand-side is called generally the Fisher information (FI).

With this review of the most interesting features of "classical" metrology, from the point of view of this thesis, we conclude this section.
In the next section, we will record some properties of the Quantum Mechanics and then we will follow with another s with the basis of Quantum Metrology.

\subsection{Quantum Mechanics from metrology perspective}

The ubiquitous probabilistic nature of Quantum Mechanics forces us to use probabilities on a regular basis.
Moreover, if one works on fields connected with experiments or some short of physical realizations this probabilistic nature of Quantum Mechanics is enhanced.
On the other hand, exotic features such as entanglement arise from Quantum Mechanics, which give us unique opportunities to exploit them as resources.
The present section is intended to describe these quantum phenomena in more or less detail.

\subsubsection{The quantum state, multiparticle state, entanglement}

A formal mathematical description of the quantum state is given next.
This also allows us to introduce some notation used through the thesis.
A \emph{state} in Quantum Mechanics lives on a Hilbert space, $\mathcal{H}$.
The system state, $\rho$, has the following properties:
\begin{enumerate}
  \item
  It is Hermitian, so it is invariant under the complex transposition, $\rho=\rho^\dagger$ and all its eigenvalues are real.
  \item Its trace is equal to one, $\tr(\rho)=1$.
  \item It is positive semi-definite, i.e, all its eigenvalues are bigger or equal to zero, $\rho=\sum_{\lambda}p_\lambda \Pi_\lambda$ where $p_\lambda\geq 0$ and $\Pi_\lambda\equiv\ketbra{\lambda}{\lambda}$ is the projector of the eigenstate $\ket{\lambda}$.
  From (ii), it follows that $\sum_\lambda p_\lambda = 1$.
  \item If all $p_\lambda$ are zero except one, the state is called a pure state and is equivalent to the projector of such eigenstate $\rho=\Pi_\lambda\equiv\ketbra{\lambda}{\lambda}$, where $\ket{\lambda}$ denotes a pure \emph{vector} state.
  \item It follows that the quantum states form a convex set where the extreme points are pure states.
  \item An expectation value of an observable $\mathcal{O}$ is linear on the state and it is computed as $\expect{\mathcal{O}}=\tr(\mathcal{O}\rho)$.
\end{enumerate}

The composite system of $N$ different parties live in the Hilbert space $\mathcal{H} = \mathcal{H}^{(1)}\otimes\mathcal{H}^{(2)}\otimes\cdots\otimes\mathcal{H}^{(N)}$ or for short $\mathcal{H} = \bigotimes_{i=1}^N\mathcal{H}^{(i)}$, where $\otimes$ stands for tensor product.
For instance, this composite Hilbert space could be used to represent a many-particle system, in this case $N$ particles.
A \emph{separable} state in this Hilbert space can be described by
\be
  \label{eq:bg-separable-state-definition}
  \rho_{\text{sep}} = \sum_{i}p_i\rho_i^{(1)}\otimes\rho_i^{(2)}\otimes\cdots\otimes\rho_i^{(N)},
\ee
where $p_i$ are convex weights that sum to one and are equal or bigger than zero.
If not the state is said to be \emph{entangled}.
We mention it as a formal description of the entanglement \citep{}.
One may notice at this moment, that relaxing the requirements of Eq.~\eqref{eq:bg-separable-state-definition}, one can lead to different classifications of the states.
Concepts like genuine multipartite entanglement, $k$-producible states, or entanglement depth, among others, arose from weakelly constrain the Eq.~\eqref{eq:bg-separable-state-definition} \citep{}.

It is important to remark one of such classifications in order to characterize the different levels or multipartite entanglement followed in this work.
We call a state $k$-producible, if it \emph{can} be written as a mixture of the tensor product of different multiparticle states with at most $k$ particles,
\be
  \rho_{k\text{-pro}} = \sum_i p_i\rho^{(\alpha,\dots,\beta)}_i \otimes \rho^{(\gamma,\dots,\delta)}_i\otimes \dots,
\ee
where superscript indexes between parenthesis go from 1 to $N$ and denote to which parties belong the state, and where each index appears once in each sum element.
If a state cannot be written as $k$-producible, then it must be $(k+1)$-entangled.
This defines the entanglement depth, see Figure~\ref{fig:bg-separability-k-producibility-circle}.
\begin{figure}
  \centering
  \includegraphics[scale=1.4]{img/BG_separability_k_producibility_circle.pdf}
  \caption[Diagram for $k$-producibility sets]{$k$-producible states contain $k-1$-producible states. Based on the Eq.~\eqref{} one can argue that a state that cannot be written as $k$-producible must be $k+1$-entangled or must have $k+1$ entanglement depth. A separable state can always be written as 1-producible wich is its original definition.}
  \label{fig:bg-separability-k-producibility-circle}
\end{figure}
Later on those concepts will arise on the metrology framework \citep{,}.

Besides those concepts, we present a set of operators that will appear many times in all chapters, namely the angular momentum operators.
Again those definitions allow us to introduce notation used on this book.
For a single party with discrete $d$ levels and therefore $s=(d-1)/2$, the eigenvalue equation for the angular momentum projection operators are
\be
  j_l^{(n)}\ket{m}_l^{(n)} = m \ket{m}_l^{(n)}
\ee
for $m=-s,\dots,+s$,  where $l=x,y,z$ and where it is usual to omit the subscript and superscript, $l$ and $(n)$ respectively, for simplicity.
The square of the total angular momentum, $\bs{j}^2=j_x^2+j_y^2+j_z^2$, for a single party is simply
\be
  (\bs{j}^2)^{(n)}\rho^{(n)}=s(s+1)\rho^{(n)},
\ee
where $\rho^{(n)}$ is an arbitrary pure or mixed state defined on the $\mathcal{H}^{(n)}$ Hilbert space.

The collective angular momentum projection operators are defined as the sum of their respective single-party spin operators such that they are extended to the rest of the Hilbert spaces by tensor products of the identity operators defined for the rest of subspaces,
\be
  J_l = \sum_{i=1}^N \mtxid^{(1,\dots, i-1)} \otimes j_l^{(i)} \otimes \mtxid^{(i+1,\dots,N)} \equiv \sum_{i=1}^1 j_l^{(i)},
\ee
where $\mtxid$ stand for the identity operator and the last formula is the most used since it is easy to notice that the single subspace operator must extended in each case.
The square of the angular momentum projections of a many-particle system is not equal to the sum of the square angular momentum projection of each of the parties and it is obtained by
\be
  J_l^2 = \sum_{i,j}^N j_l^{(i)} j_l^{(j)} = \sum_{i=1}^N (j_l^2)^{(i)}+\sum_{i\neq j}^N j_l^{(i)} j_l^{(j)}.
\ee
Therefore, the total square angular momentum is neither the sum of all single-party square angular momentum and is computed by
\be
  J_l^2 = \sum_{i,j}^N j_l^{(i)} j_l^{(j)} = \sum_{i=1}^N (j_l^2)^{(i)}+\sum_{i\neq j}^N j_l^{(i)} j_l^{(j)},
\ee
where we separated the sum into two pieces, the first corresponds to the sum of all single party angular momentum projections squared and the second corresponds to the product of angular momentum projection operators of two distinct subsystems.
Many more combinations of these single-party operators may arise on different contexts.
In the Appendix~\ref{}, we discuss in more detail the different structures that arise from adding the angular momentum operators, e.g., the symmetric subspace or the fixed total angular momentum subspace.

Evolution:

- Markov

- Limblad

Measurements:

- POVMs

Quantum Information:

\subsection{Quantum Metrology}

We summarize important recent advances in Quantum Metrology
Simple metrological setups allow for encoding some wanted parameter into the state of the system, thus from the readout of the final state one can infer on it.
The estimation theory applied to the intrinsic probabilistic nature of quantum states has lead to the the formulation of Quantum Metrology.
Merging the probabilistic features of quantum mechanics and the estimation theory is not trivial.
Nevertheless, starting from the pioneering works of Wotters, Braunstein and others back in 19XX \citep{Wotters, Braunstein}, until the works of Giovannetti et al, Paris and others roughly 20 years later \citep{Giovannetti, Paris2009}, the field of Quantum Metrology has been stabilized with solid foundations.
Later, advanced works in Quantum Metrology appeared \citep{} together with experimental realizations \citep{} which raised the interest in this topic.
In this section, we will highlight the most important aspects of this field and with this we will conclude this introductory chapter.

The most basic scheme for a the metrological setup in this context is the following.
First, a state is prepared followed by the evolution in which the unknown parameter $\theta$ is imprinted into the state.
Finally, the outgoing state is characterized by some measurement $M$ which allows to infer in the value of the parameter $\theta$.
The details of the evolution, the initial state and the post-characterization using $M$ or a set of different measurements, is crucial for succeed.
Figure~\ref{} illustrates the main steps of Quantum Metrology.
\begin{figure}
  \centering
  \includegraphics[scale=1.4]{img/BG_preparation_encoding_estimation.pdf}
  \caption[Quantum Metrology estimation process]{Sequence of the different steps for the basics of the estimation process on Quantum Metrology. First, an input state $\rho$ enters the region on which the unknown parameter $\theta$ is imprinted on it, for the most general case, represented with $\Lambda_{\Theta}$. Last, the state that has encoded the parameter $\theta$ on it is measured and $\theta$ must be inferred from the measured quantity $\expect{M}$.}
  \label{fig:bg-preparation-encoding-estimation}
\end{figure}

In the many-particle case, most of the metrology experiments have been done in systems with simple Hamiltonians that do not contain interaction terms.
Such Hamiltonians cannot create entanglement between the particles.
A typical situation, is that we rotate our many particle state by some angle and we want to estimate the rotation angle $\theta$.
It has been shown that particles exhibiting quantum correlations, or more precisely, quantum entanglement \citep{Guehne2009, Luis2004}, provide a higher precision than an ensemble of not entangled particles.
The most important question is how the achievable precision of the angle estimation $\varian{\theta}$ scales with the number of particles.
Very general derivations lead to, at best,
\be
  \label{eq:bg-shot-noise-scaling}
  \varian{\theta}\sim \frac{1}{N}
\ee
for nonentangled particles.
The equation above takes the name \emph{shot-noise scaling} (SNS), the term originating from the shot-noise in electronic circuits, which is due to the discrete nature of the electric charge.
On the other hand, quantum entanglement makes it possible to reach
\be
  \label{eq:bg-heisenberg-scaling}
  \varian{\theta}\sim \frac{1}{N^2}
\ee
which is called the \emph{Heisenberg scaling} (HS).
Note that if the Hamiltonian of the dynamics has interaction terms, then these bounds can be surpassed \citep{Luis2004, Napolitano2011, Boixo2007, Braun2011, Roy2008, Choi2008, Rey2007}.

It is time to mention that the above calculations have been carried out for an ideal situation.
When an uncorrelated noise is present in the system, it turns out that for large enough particle number the scaling becomes a SNS \citep{}.
The possible survival of a better scaling under correlated noise, under particular circumstances, or depending on some interpretation of the metrological task, is at the center of attention currently.
All these are strongly connected to the question of whether strong multipartite entanglement can survive in a noisy environment.

Finally, note that often, instead of $\varian{\theta}$ one calculates its inverse, which is large for high precision.
It scales as $\varinv{\theta}\sim N$ for SNS and as $\varinv{\theta}\sim N^2$ for the HS.

\subsubsection{Quantum Magnetometry}
\label{sec:bg-quantum-metro}

Without loss of generality, we present in this section the characterization of the precision of one of the simplest metrological task, namely the estimation of a homogeneous magnetic field based in the interaction between the system and the field.
In this section, we will mostly study the interaction of a system with a homogeneous field in the $z$-direction.
In the Chapter~\ref{sec:gm}, we will show a different situation on which the magnetic field changes linearly with the position of the system.
With the aim of estimating the strength of the magnetic field, a prove state is used in order to interact with it, coupling the magnetic moment of the state and the external magnetic field.
Finally, measuring how the state has changed one could in principle infer on the strength of the magnetic field.

In general, we will say that the magnetic moments of the states come exclusively from the spin angular momentum, neglecting any possible contribution from the orbital angular momentum.
This way the physics is simpler.
This is justified in the sense that most of the recent experiments on this context have been carried out with ion-traps, BECs or at most cold atomic ensembles, which have indeed a negligible orbital angular momentum.

Beside this considerations, the interaction Hamiltonian can be written as,
\be
  H = - \bs{\mu} \cdot \bs{B}
\ee
up to some constant factor.
Now in the simplest case we will choose the magnetic field to be pointing to some fixed direction, for instance, the $z$-direction.
So the magnetic field vector can be written as ${B}=B\bs{k}$, where $\bs{k}$ is the unitary vector pointing to the $z$-direction.
This way estimation problem is much more simple, since one has not to determine the direction of the magnetic field.

The magnetic moment of the system is proportional to the spin angular momentum, $\bs{\mu}=-\mu_\text{B} g_{\text{s}}\hbar^{-1} \bs{J}$, where $\mu_{\text{B}}$ and $g_{\text{s}}$ are the Bohr magneton and anomalous gyromagnetic factor respectively.
Finally, one can rewrite the interaction Hamiltonian as,
\be
  \label{eq:bg-hamiltonian-homogeneous-field}
  H = \gamma B J_z
\ee
where $\gamma = \mu_\text{B} g_{\text{s}}\hbar^{-1}$ and we have used the fact that $\bs{J}\cdot\bs{k}=J_z$.
Finally, the unitary operator leading the evolution of the system can be written as,
\be
  \label{eq:bg-unitary-homogeneous-field}
  U = \exp(-i\theta J_z),
\ee
where the magnetic field strength is encoded into the phase-shift $\theta=-\mu_\text{B} g_\text{s} t B/\hbar$.
Here $\mu_\text{B}$ stand for the Magneton of Bhor and $g_\text{s}$ for the giro-magnetic constant for the spin angular momentum, and $t$ is the time spent on the evolution.

We have to mention as well that for large particle ensembles, typically only collective quantities can be measured in order to characterize the state and read the outgoing system.
Such collective quantities are in this case the angular momentum components defined in Eq.~\eqref{} and their combinations.
More concretelly, we can measure the expectation values of any
\be
  J_{\bs{n}}:=\sum_{l=x,y,z} n_l J_l,
\ee
where $\bs{n}=(n_x,n_y,n_z)$ is a unit vector describing the direction of the component.

\subsubsection[Metrology with almost polarized states]{Metrology with almost polarized states, including spin-squeezed states}

Let us present one of the basic approaches to metrological precision of a quantum setup.
In order to estimate the phase-shift $\theta$ we measure the expectation value of a Hermitian operator, which we will denote here by $M$ in the following.
If the evolution time is a constant then estimating $\theta$ is equivalent to estimating the magnetic field strength $B$ in Eq.~\eqref{eq:bg-hamiltonian-homogeneous-field}.
The precision of the estimation can be characterized with the error propagation formula as
\be
  \label{eq:bg-error-propagation-formula}
  \varian{\theta} = \frac{\varian{M}}{|\partial_\theta\expect{M}|^2},
\ee
where $\expect{M}$ is the expectation value of the operator $M$, and we used the assumption that $\expect{M}$ is a random variable and that it has a PDF that can relate $\prob(\expect{M}|\theta)$ with a PDF as a function of the estimator $\prob(\hat{\theta}|\theta)$, see the Section~\ref{} and for a recent review discussing this approach in detail, see Ref.~\citep{MR38}.
Thus, the precision of the estimate depends on how sensitive $\expect{M}$ is to the change of $\theta$, and also on how large the variance of $M$ is.
Based on the formula Eq.~\eqref{eq:bg-error-propagation-formula}, one can see that the larger the slope $|\partial_\theta \expect{M}|$, the higher the precision.
On the other hand, the larger the variance $\varian{M}$, the lower the precision.
Figure~\ref{} helps to interpret the quantities appearing in Eq.~\eqref{eq:bg-error-propagation-formula}.
\begin{figure}
  \centering
  \includegraphics[scale=1.4]{img/BG_expect_m_evo.pdf}
  \caption[Graphical explanation of the error-propagation formula]{(Blue solid) Functional relation between the expectation value $\expect{M}$ and the estimator of the wanted parameter $\hat\theta$.
  (Green dashed) One to one correspondence when the estimator $\hat{\theta}$ is based on $\expect{M}$. (Red dotted) Obtaining the error of the estimate is based on Eq.~\eqref{eq:bg-error-propagation-formula}.
  The slope of the curve at that point, denoted with $\tan\alpha$, directly relates the uncertainty $\sigma_M$ on the measured quantity $\expect{M}$ and the error on the estimation $\sigma_\theta$.}
  \label{fig:bg-expect-m-evo}
\end{figure}

At this point, we use the formula Eq.~\eqref{eq:bg-hamiltonian-homogeneous-field} to compute and this way highlight special signatures of interesting systems and their metrological behavior.
We center our attention into multi-partite systems of spin-$\frac{1}{2}$ particles, widely known in the field as qubits, for homogeneous magnetometry in order to simplify the discussion.
For this, let us start with an almost polarized state.
The spin vector of the ensemble, originally pointing into the $y$-direction, will be rotated.
The rotation after the evolution is used to estimate the strength of the magnetic field.
A way to measure the rotation suffered by the system is to measure the expectation value $\expect{J_x}$, which is zero at the beginning.

For small angles of $\theta$ and using the Eq.~\eqref{eq:bg-error-propagation-formula} after substituting $M$ by $J_x$, we arrive to
\be
  \label{eq:bg-error-propagation-measuring-jx}
  \varinv{\theta} = \frac{|\partial_\theta \expect{J_x}|^2}{\varian{J_x}},
\ee
for state almost polarized along the $y$-axis.
Next, we have to compute these values for small angles of $\theta$.

To compute them we use the Heisenberg picture of the operator $J_x$, that after applying the unitary operator Eq.~\eqref{eq:bg-unitary-homogeneous-field}, is written as
\be
  J_x(\theta) = U^{i\theta J_z} J_x U^{-i\theta J_z} = \coss{\theta}J_x -  \sins{\theta} J_y,
\ee
where we also introduced a notation for trigonometric functions since they will appear many times in this work, $\coss{x} \rightarrow \cos(x)$, $ \sins{x}\rightarrow\sin(x)$ and $\tans{x}\rightarrow\tan(x)$.
Thus, the square of $J_x$ is simply
\be
  J_x^2(\theta) = \coss{\theta}^2J_x^2 -2 \coss{\theta}\sins{\theta}\{J_x,J_y\}+  \sins{\theta}^2 J_y^2,
\ee
where $\{A,B\}=AB+BA$ is the anti-commutator, since the operators may not commute in general and in this particular case it is not an exception.
Finally we compute the derivative with respect to $\theta$ of $\expect{J_x}$ as
\be
  \partial_{\theta}\expect{J_x}(\theta)=\partial_{\theta}\coss{\theta}\expect{J_x} - \partial_{\theta}\sins{\theta}\expect{J_y} = -\sins{\theta}\expect{J_x}-\coss{\theta}\expect{J_y},
\ee
where we used the linearity of the trace to compute the expectation value and the linearity of the derivative.

Hence, assuming we are in the small angle limit to compute the Eq.~\eqref{eq:bg-error-propagation-measuring-jx} we may take the terms proportional to $\sin(\theta)$ as zero.
Therefore, when we measure $\expect{J_x}$ to estimate the angle $\theta$, the precision of the estimation is given by
\be
  \label{eq:bg-error-propagation-measuring-jx-computed}
  \varinv{\theta} = \frac{|\coss{\theta}\expect{J_y}|^2}{\coss{\theta}^2\expect{J_x^2} - (\coss{\theta}\expect{J_x})^2} = \frac{\expect{J_y}^2}{\varian{J_x}}.
\ee

For the totally polarized state along the $y$-axis, the initial expectation values $\expect{J_x}$, $\expect{J_x^2}$ and $\expect{J_y}$ needed to compute the precision are
\be
  \begin{split}
    \expect{J_x}_{\text{tp}}  & = 0,\\
    \expect{J_x^2}_{\text{tp}}  & = \tfrac{N}{4},\\
    \expect{J_y}_{\text{tp}}  & = \tfrac{N}{2}.
  \end{split}
\ee
Thus, we obtain a precision that scales linearly with $N$,
\be
  \varinv{\theta}_{\text{tp}} = \frac{N^2/4}{N/4} = N.
\ee
Notice that the totally polarized state is a separable pure state with all particles pointing into the $y$-direction.
We obtained the shot-noise scaling, even with very simple, qualitative arguments

A way of improving the precision is considering that the variances of the angular momentum components are bounded by the Heisenberg uncertainty relation \citep{MR41}
\be
  \varian{J_x}\varian{J_z} \geq \frac{1}{4}|\expect{J_y}|.
\ee
Due to decreasing $\varian{J_x}$, our state fulfills
\be
  \varian{J_x}<\frac{1}{2}|\expect{J_y}|,
\ee
where the main spin point along the $y$-axis and for totally polarized states $\varian{J_x}=\frac{1}{2}|\expect{J_y}|$.
Such states are called \emph{spin-squeezed} states \citep{MR41, MR42, MR43, MR44} and they can in principal bite the SNS.

Next, we can ask, what the best possible phase estimation precision is for the metrological task considered in this section.
For that, we have to use the following inequality based on general principles of angular momentum theory
\be
  \label{eq:bg-total-angular-momentum-saturability}
  \expect{J_x^2+J_y^2+J_z^2} \leq \frac{N(N+2)}{4}.
\ee
Note that Eq.~\eqref{eq:bg-total-angular-momentum-saturability} is saturated only by symmetric multiparticle states, see Appendix~\ref{}.
Together with the identity connecting the second moments, variances and expectation values
\be
  \varian{J_l} + \expect{J_l}^2 = \expect{J_l^2},
\ee
Eq.~\eqref{eq:bg-total-angular-momentum-saturability} leads to a bound on the uncertainty in the anti-squeezed direction, in this case $z$-direction, for the variance
\be
  \varian{J_z} \leq \frac{N(N+2)}{4} - \expect{J_y}^2 = \frac{N}{2} + \frac{N^2}{4}\lpar 1 - \frac{\expect{J_y}^2}{J_{\max}^2}\rpar,
\ee
where $J_{\max}$ is the maximum value a angular momentum component can take, i.e., $N/2$.
This leads to a bound in the precision when measuring $\expect{J_x}$ as
\be
  \varinv{\theta} = \frac{\expect{J_y}^2}{\varian{J_x}}\leq 4\varian{J_z}\leq 2N + N^2\lpar 1 - \frac{\expect{J_y}^2}{j_{\max}}\rpar,
\ee
which indicates that the precision is limited for almost completely polarized states to the shot-noise scaling.
The bound above is not optimal, as for the fully polarized state we would expect $N$ and we obtain $2N$.

\subsubsection{The quantum Fisher information}

Let us now change our discourse drastically.
In this section, we review the theoretical background of the Fisher information, the Cram\'er-Rao bound and we introduce the quantum Fisher information.

First of all, we have already shown the Fisher information and the Cram\'er-Rao bound in the Section~\ref{}.
The formula Eq.~\eqref{eq:bg-classical-cr-bound-and-fi} tells us that if we measure an operator, say $M$, and we know its probability distribution function $\prob(\expect{M}|\theta)$, we can bound the achievable precision.
In quantum mechanics the PDF of a state $\rho_\theta$ when measuring the operator $M$ is given by
\be
  \prob(m|\theta) = \tr({\Pi_{m}\rho_\theta}),
\ee
where $\Pi_{m}$ are the projector operators or the eigenstates on which $M$ is expanded, $M = \sum m \Pi_m$.
Following arguments found on Refs.~\citep{Paris2009, }, one can reach to an universal bound valid for any kind of measurements.
This bound is called the quantum Cram\'er-Rao bound and in one site is the achievable precision and in the other site is the quantum Fisher information (QFI),
\be
  \varinv{\theta} \leq \qfi[\rho,J_z].
\ee


\be
  \label{eq:bg-pezze-bound}
  \qfi[\rho,J_z] \geq \frac{\expect{J_y}^2}{\varian{J_x}}
\ee

The QFI
